{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc69ca5",
   "metadata": {},
   "source": [
    "# SC3000/CZ3005 Assignment 1 - Balancing a Pole on a Cart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0569f",
   "metadata": {},
   "source": [
    "Submission Deadline: 11:59 PM, 5 April 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b0783",
   "metadata": {},
   "source": [
    "## Group Members and Contributions:\n",
    "\n",
    "1. Bryan Toh Wee Sheng\n",
    "2. Lim Shaojun\n",
    "3. Keith Lim En Kai (U2220506C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36572ef7",
   "metadata": {},
   "source": [
    "# 1. Problem description: (Taken from Assignment Document)\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on thecart and the goal is to balance the pole by applying forces in the left and right direction on the cart. In this project, you will need to develop a Reinforcement Learning (RL) agent. The trained agent makes the decision to push the cart to the left or right based on the cart position, velocity, and the pole angle, angular velocity. (4 parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e883c",
   "metadata": {},
   "source": [
    "![Alternative Text](https://www.gymlibrary.dev/_images/cart_pole.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd86c998",
   "metadata": {},
   "source": [
    "## 1.1 Problem Instance\n",
    "You are given an instance of the cart pole environment implemented by the gym library. As with any good solution to a problem, we start with \n",
    "\n",
    "### Action Space:\n",
    "The action is an nd array with shape (1, which can take values {0,1} indicating actions pushing the cart to the left or right respectively. Note that the velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it.\n",
    "\n",
    "### Observation Space:\n",
    "The observation is an nd array with shape (4,) with the values corresponding to the following positions and velocities:\n",
    "\n",
    "| Num | Observation | Min | Max |\n",
    "|---|---|---|---|\n",
    "| 1 | Cart Position | -4.8 | 4.8 |\n",
    "| 2 | Cart Velocity | -inf | inf |\n",
    "| 3 | Pole Angle | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "| 4 | Pole Angular Velocity | -inf | inf |\n",
    "\n",
    "### Reward:\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted.\n",
    "\n",
    "### Starting State:\n",
    "All observations are assigned a uniformly random value in **(-0.05, 0.05)**\n",
    "\n",
    "### Episode End:\n",
    "The episode ends if any one of the following occurs:\n",
    "1. Termination: Pole Angle is greater than +/- 12 Degrees == 0.209 rad\n",
    "2. Termination: Cart Position is greater than +/- 2.4 (center of the cart reaches the edge of the display)\n",
    "3. Truncation: Episode length is greater than 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163981e4",
   "metadata": {},
   "source": [
    "# 2. Requirements and Guidelines\n",
    "## 2.1 Tasks and Marking Criteria\n",
    "\n",
    "### Task 1: Development of an Reinforcement Learning (RL) Agent. (30 Marks)\n",
    "Demonstrate the correctness of the implementation by sampling a random state from the cart pole environment, inputting to the agent, and outputting a chosen action. Print the values of the state and chosen action in the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe5c8b",
   "metadata": {},
   "source": [
    "### Task 2: Demonstrate the effectiveness of the RL Agent (40 Marks)\n",
    "Run for 100 episodes (reset the enviroment at the beginning of each episode) and plot the cumulative reward against all episodes in the Jupyter Notebook. Print the average reward over the 100 episodes. The average reward should be larger than **195**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c27b1",
   "metadata": {},
   "source": [
    "### Task 3: Render one episode played by the developed RL agent on the Jupyter Notebook (10 Marks)\n",
    "Please refer to the sample code link for rendering code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc1ef5",
   "metadata": {},
   "source": [
    "### Task 4: Format the Jupyter Notebook by including step-by-step instructions and explanations, such that the notebook is easy to follow and run (20 Marks)\n",
    "Include text explanation to demonstrate the originality of your implementation and your understanding of the code. For example, for each task, explain your approach and analyze the output; if you improve an exisiting approach, explain your improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86076dad",
   "metadata": {},
   "source": [
    "## 2.2 Output Format\n",
    "All codes are to be included in a single Jupyter notebook written in Python (i.e., .ipynb file).\n",
    "1.\tInclude all codes for Task 1-4. Note that the submission is invalid if it only contains the outputs and plots without codes to obtain it.\n",
    "2.\tRun the notebook before submission to save the output in the notebook, i.e., by opening the ipynb file (without running it), one can see the outputs and plots for Task 1-3\n",
    "3.\tMake sure the Jupyter notebook is runnable, i.e., by running each code block sequentially from top to bottom, one can get the results for Task 1-3. The TAs may run your notebook.\n",
    "4.\tUnless you are experienced with Jupyter, it is recommended to modify from the provided Jupyter notebook sample, rather than creating a new one.\n",
    "5.\tIf the developed RL agent is a trainable neural network, submit a .zip file by zipping the trained model parameters (e.g., .pth for PyTorch) and the ipynb file. In this case, your notebook must include the training code and model loading code.\n",
    "6.\tContribution: Please clearly state the contribution of each team member in the beginning of the Jupyter notebook if you have more than one member in your team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9335ad",
   "metadata": {},
   "source": [
    "## Installing Required Dependencies and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62a8eca",
   "metadata": {},
   "source": [
    "### Crafting the Cartpole Enviroment (gym) and analysing action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d1913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print('The action space consists of', env.action_space.n, 'actions, left and right represented by 0 and 1 respectively.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a8ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84da9be",
   "metadata": {},
   "source": [
    "The observation space is given above. The first two arrays define the min and max values of the 4 observed values, corresponding to cart position, velocity and pole angle, angular velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "print(\"Initial observations:\", observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.step(0)[:4]\n",
    "# print(env.step(0))\n",
    "print(\"New observations after choosing action 0:\", observation)\n",
    "print(\"Reward for this step:\", reward)\n",
    "print(\"Is this round done?\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "cumulative_reward = 0\n",
    "done = False\n",
    "while not done:\n",
    "    observation, reward, done, info = env.step(0)[:4]\n",
    "    cumulative_reward += reward\n",
    "print(\"Cumulative reward for this round:\", cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de297a8",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be8aa5",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Agent (Using _______)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(observation):\n",
    "    return random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "action = random_agent(observation)\n",
    "print(\"Observation:\", observation)\n",
    "print(\"Chosen action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f969149",
   "metadata": {},
   "source": [
    "### Task 1 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "random_state = env.observation_space.sample()\n",
    "chosen_action = random_agent(random_state)\n",
    "\n",
    "# [0]: Cart Position, [1]: Cart Velocity, [2]: Pole Angle, [3]: Pole Angular Velocity\n",
    "print(\"Random state: \", random_state)\n",
    "print(\"Chosen action: \", chosen_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c049ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.step(chosen_action)[:4]\n",
    "print(\"New observations after choosing the chosen action:\", observation)\n",
    "print(\"Reward for this step:\", reward)\n",
    "print(\"Is this round done?\", done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251989e8",
   "metadata": {},
   "source": [
    "Explanation: We used a sample from the observation space and inputted it into the policy agent to determine the action to be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24f268",
   "metadata": {},
   "source": [
    "## Task 2: Demonstrate the effectiveness of the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_results = np.random.randint(150, 250, size=100)\n",
    "plt.plot(episode_results)\n",
    "plt.title('Cumulative reward for each episode')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.xlabel('episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eac3b7",
   "metadata": {},
   "source": [
    "Printing out the average reward over the 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2bc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cumulative reward:\", episode_results.mean())\n",
    "print(\"Is my agent good enough?\", episode_results.mean() > 195)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae108c43",
   "metadata": {},
   "source": [
    "### Task 2 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(agent, num_episodes):\n",
    "    episode_rewards = []\n",
    "    for i, episode in enumerate(range(num_episodes)):\n",
    "        total_reward = 0\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            chosen_action = agent(observation)\n",
    "            observation, reward, done, info = env.step(chosen_action)[:4]\n",
    "            total_reward += reward\n",
    "#             print(f\"Episode {i} Score:\", reward)\n",
    "        \n",
    "        \n",
    "        print(f\"Total reward: {total_reward}\")\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6226f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "episode_rewards = run_episodes(random_agent, num_episodes)\n",
    "\n",
    "# Plot cumulative reward against episodes\n",
    "plt.plot(np.arange(1, num_episodes + 1), np.cumsum(episode_rewards))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Reward over 100 Episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = np.mean(episode_rewards)\n",
    "print(f\"Average reward over {num_episodes} Episodes: {average_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bcb3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if average_reward > 195:\n",
    "    print(\"The average reward is > 195\")\n",
    "else:\n",
    "    print(\"The average reward did not meet the required threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db4544",
   "metadata": {},
   "source": [
    "## Task 3: Render one episode played by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4775a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ce6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import glob\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "\n",
    "\n",
    "# env = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n",
    "# observation = env.reset()\n",
    "# while True:\n",
    "#     env.render()\n",
    "#     action = RL_agent(observation)\n",
    "#     observation, reward, done, info = env.step(action) [:4]\n",
    "#     if done: \n",
    "#       break\n",
    "# env.close()\n",
    "# show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cfa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from IPython import display\n",
    "from gym.wrappers import RecordVideo\n",
    "# from gym.wrappers import TimeLimit\n",
    "# from gym.wrappers.monitoring import video_recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b54c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = TimeLimit(gym.make(\"CartPole-v1\"), max_episode_steps=500)\n",
    "# env = video_recorder.VideoRecorder(env, \"./video\")\n",
    "# observation = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf56a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_episode(RL_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821cf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    #your agent goes here\n",
    "    action = random_agent(observation)\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    if done: \n",
    "      break;    \n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed815687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
