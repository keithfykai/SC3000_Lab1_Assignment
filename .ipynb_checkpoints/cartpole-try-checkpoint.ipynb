{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc69ca5",
   "metadata": {},
   "source": [
    "# SC3000/CZ3005 Assignment 1 - Balancing a Pole on a Cart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0569f",
   "metadata": {},
   "source": [
    "Submission Deadline: 11:59 PM, 5 April 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b0783",
   "metadata": {},
   "source": [
    "## Group Members and Contributions:\n",
    "\n",
    "1. Bryan Toh Wee Sheng\n",
    "2. Lim Shaojun\n",
    "3. Keith Lim En Kai (U2220506C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36572ef7",
   "metadata": {},
   "source": [
    "# 1. Problem description: (Taken from Assignment Document)\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on thecart and the goal is to balance the pole by applying forces in the left and right direction on the cart. In this project, you will need to develop a Reinforcement Learning (RL) agent. The trained agent makes the decision to push the cart to the left or right based on the cart position, velocity, and the pole angle, angular velocity. (4 parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e883c",
   "metadata": {},
   "source": [
    "![Alternative Text](https://www.gymlibrary.dev/_images/cart_pole.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd86c998",
   "metadata": {},
   "source": [
    "## 1.1 Problem Instance\n",
    "You are given an instance of the cart pole environment implemented by the gym library. As with any good solution to a problem, we start with \n",
    "\n",
    "### Action Space:\n",
    "The action is an nd array with shape (1, which can take values {0,1} indicating actions pushing the cart to the left or right respectively. Note that the velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it.\n",
    "\n",
    "### Observation Space:\n",
    "The observation is an nd array with shape (4,) with the values corresponding to the following positions and velocities:\n",
    "\n",
    "| Num | Observation | Min | Max |\n",
    "|---|---|---|---|\n",
    "| 1 | Cart Position | -4.8 | 4.8 |\n",
    "| 2 | Cart Velocity | -inf | inf |\n",
    "| 3 | Pole Angle | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "| 4 | Pole Angular Velocity | -inf | inf |\n",
    "\n",
    "### Reward:\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted.\n",
    "\n",
    "### Starting State:\n",
    "All observations are assigned a uniformly random value in **(-0.05, 0.05)**\n",
    "\n",
    "### Episode End:\n",
    "The episode ends if any one of the following occurs:\n",
    "1. Termination: Pole Angle is greater than +/- 12 Degrees == 0.209 rad\n",
    "2. Termination: Cart Position is greater than +/- 2.4 (center of the cart reaches the edge of the display)\n",
    "3. Truncation: Episode length is greater than 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163981e4",
   "metadata": {},
   "source": [
    "# 2. Requirements and Guidelines\n",
    "## 2.1 Tasks and Marking Criteria\n",
    "\n",
    "### Task 1: Development of an Reinforcement Learning (RL) Agent. (30 Marks)\n",
    "Demonstrate the correctness of the implementation by sampling a random state from the cart pole environment, inputting to the agent, and outputting a chosen action. Print the values of the state and chosen action in the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe5c8b",
   "metadata": {},
   "source": [
    "### Task 2: Demonstrate the effectiveness of the RL Agent (40 Marks)\n",
    "Run for 100 episodes (reset the enviroment at the beginning of each episode) and plot the cumulative reward against all episodes in the Jupyter Notebook. Print the average reward over the 100 episodes. The average reward should be larger than **195**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c27b1",
   "metadata": {},
   "source": [
    "### Task 3: Render one episode played by the developed RL agent on the Jupyter Notebook (10 Marks)\n",
    "Please refer to the sample code link for rendering code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc1ef5",
   "metadata": {},
   "source": [
    "### Task 4: Format the Jupyter Notebook by including step-by-step instructions and explanations, such that the notebook is easy to follow and run (20 Marks)\n",
    "Include text explanation to demonstrate the originality of your implementation and your understanding of the code. For example, for each task, explain your approach and analyze the output; if you improve an exisiting approach, explain your improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86076dad",
   "metadata": {},
   "source": [
    "## 2.2 Output Format\n",
    "All codes are to be included in a single Jupyter notebook written in Python (i.e., .ipynb file).\n",
    "1.\tInclude all codes for Task 1-4. Note that the submission is invalid if it only contains the outputs and plots without codes to obtain it.\n",
    "2.\tRun the notebook before submission to save the output in the notebook, i.e., by opening the ipynb file (without running it), one can see the outputs and plots for Task 1-3\n",
    "3.\tMake sure the Jupyter notebook is runnable, i.e., by running each code block sequentially from top to bottom, one can get the results for Task 1-3. The TAs may run your notebook.\n",
    "4.\tUnless you are experienced with Jupyter, it is recommended to modify from the provided Jupyter notebook sample, rather than creating a new one.\n",
    "5.\tIf the developed RL agent is a trainable neural network, submit a .zip file by zipping the trained model parameters (e.g., .pth for PyTorch) and the ipynb file. In this case, your notebook must include the training code and model loading code.\n",
    "6.\tContribution: Please clearly state the contribution of each team member in the beginning of the Jupyter notebook if you have more than one member in your team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9335ad",
   "metadata": {},
   "source": [
    "## Installing Required Dependencies and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install moviepy\n",
    "# !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "# !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "# !pip install gym[classic_control]\n",
    "# !apt-get update > /dev/null 2>&1\n",
    "# !apt-get install cmake > /dev/null 2>&1\n",
    "# !pip install --upgrade setuptools 2>&1\n",
    "# !pip install ez_setup > /dev/null 2>&1\n",
    "# import gym\n",
    "# from gym import logger as gymlogger\n",
    "# from gym.wrappers import RecordVideo\n",
    "# gymlogger.set_level(40) #error only\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import math\n",
    "# import glob\n",
    "# import io\n",
    "# import base64\n",
    "# from IPython.display import HTML\n",
    "# from IPython import display as ipythondisplay\n",
    "\n",
    "# def show_video():\n",
    "#   mp4list = glob.glob('video/*.mp4')\n",
    "#   if len(mp4list) > 0:\n",
    "#     mp4 = mp4list[0]\n",
    "#     video = io.open(mp4, 'r+b').read()\n",
    "#     encoded = base64.b64encode(video)\n",
    "#     ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "#                 loop controls style=\"height: 400px;\">\n",
    "#                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "#              </video>'''.format(encoded.decode('ascii'))))\n",
    "#   else: \n",
    "#     print(\"Could not find video\")\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "\n",
    "    # Hidden layer with 256 nodes\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    \n",
    "    # Hidden layer with 64 nodes\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        # by default, CartPole-v1 has max episode steps = 500\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create main model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.train_start:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # Standard - DQN\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                # selection and evaluation of action is on the target Q Network\n",
    "                # Q_max = max_a' Q_target(s', a')\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "            \n",
    "    def run(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                if not done or i == self.env._max_episode_steps-1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                if done:                   \n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.EPISODES, i, self.epsilon))\n",
    "                    if i == 500:\n",
    "                        print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                        self.save(\"cartpole-dqn.h5\")\n",
    "                        return\n",
    "                self.replay()\n",
    "\n",
    "    def test(self):\n",
    "        self.load(\"cartpole-dqn.h5\")\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
    "                    break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = DQNAgent()\n",
    "    agent.run()\n",
    "    #agent.test()\n",
    "\n",
    "def run(self):\n",
    "    for e in range(self.EPISODES):\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        done = False\n",
    "        i = 0\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = self.act(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, self.state_size])\n",
    "            if not done or i == self.env._max_episode_steps-1:\n",
    "                reward = reward\n",
    "            else:\n",
    "                reward = -100\n",
    "            self.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            i += 1\n",
    "            if done:                   \n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.EPISODES, i, self.epsilon))\n",
    "                if i == 500:\n",
    "                    print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                    self.save(\"cartpole-dqn.h5\")\n",
    "                    return\n",
    "            self.replay()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62a8eca",
   "metadata": {},
   "source": [
    "### Crafting the Cartpole Enviroment (gym) and analysing action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d1913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print('The action space consists of', env.action_space.n, 'actions, left and right represented by 0 and 1 respectively.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a8ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84da9be",
   "metadata": {},
   "source": [
    "The observation space is given above. The first two arrays define the min and max values of the 4 observed values, corresponding to cart position, velocity and pole angle, angular velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "print(\"Initial observations:\", observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.step(0)[:4]\n",
    "# print(env.step(0))\n",
    "print(\"New observations after choosing action 0:\", observation)\n",
    "print(\"Reward for this step:\", reward)\n",
    "print(\"Is this round done?\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "cumulative_reward = 0\n",
    "done = False\n",
    "while not done:\n",
    "    observation, reward, done, info = env.step(0)[:4]\n",
    "    cumulative_reward += reward\n",
    "print(\"Cumulative reward for this round:\", cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de297a8",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be8aa5",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Agent (Using _______)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(observation):\n",
    "    return random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "action = random_agent(observation)\n",
    "print(\"Observation:\", observation)\n",
    "print(\"Chosen action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f969149",
   "metadata": {},
   "source": [
    "### Task 1 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "random_state = env.observation_space.sample()\n",
    "chosen_action = random_agent(random_state)\n",
    "\n",
    "# [0]: Cart Position, [1]: Cart Velocity, [2]: Pole Angle, [3]: Pole Angular Velocity\n",
    "print(\"Random state: \", random_state)\n",
    "print(\"Chosen action: \", chosen_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c049ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.step(chosen_action)[:4]\n",
    "print(\"New observations after choosing the chosen action:\", observation)\n",
    "print(\"Reward for this step:\", reward)\n",
    "print(\"Is this round done?\", done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251989e8",
   "metadata": {},
   "source": [
    "Explanation: We used a sample from the observation space and inputted it into the policy agent to determine the action to be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24f268",
   "metadata": {},
   "source": [
    "## Task 2: Demonstrate the effectiveness of the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_results = np.random.randint(150, 250, size=100)\n",
    "plt.plot(episode_results)\n",
    "plt.title('Cumulative reward for each episode')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.xlabel('episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eac3b7",
   "metadata": {},
   "source": [
    "Printing out the average reward over the 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2bc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cumulative reward:\", episode_results.mean())\n",
    "print(\"Is my agent good enough?\", episode_results.mean() > 195)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae108c43",
   "metadata": {},
   "source": [
    "### Task 2 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(agent, num_episodes):\n",
    "    episode_rewards = []\n",
    "    for i, episode in enumerate(range(num_episodes)):\n",
    "        total_reward = 0\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            chosen_action = agent(observation)\n",
    "            observation, reward, done, info = env.step(chosen_action)[:4]\n",
    "            total_reward += reward\n",
    "#             print(f\"Episode {i} Score:\", reward)\n",
    "        \n",
    "        \n",
    "        print(f\"Total reward: {total_reward}\")\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6226f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "episode_rewards = run_episodes(random_agent, num_episodes)\n",
    "\n",
    "# Plot cumulative reward against episodes\n",
    "plt.plot(np.arange(1, num_episodes + 1), np.cumsum(episode_rewards))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Reward over 100 Episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = np.mean(episode_rewards)\n",
    "print(f\"Average reward over {num_episodes} Episodes: {average_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bcb3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if average_reward > 195:\n",
    "    print(\"The average reward is > 195\")\n",
    "else:\n",
    "    print(\"The average reward did not meet the required threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db4544",
   "metadata": {},
   "source": [
    "## Task 3: Render one episode played by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4775a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ce6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import glob\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "\n",
    "\n",
    "# env = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n",
    "# observation = env.reset()\n",
    "# while True:\n",
    "#     env.render()\n",
    "#     action = RL_agent(observation)\n",
    "#     observation, reward, done, info = env.step(action) [:4]\n",
    "#     if done: \n",
    "#       break\n",
    "# env.close()\n",
    "# show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cfa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from IPython import display\n",
    "from gym.wrappers import RecordVideo\n",
    "# from gym.wrappers import TimeLimit\n",
    "# from gym.wrappers.monitoring import video_recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b54c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = TimeLimit(gym.make(\"CartPole-v1\"), max_episode_steps=500)\n",
    "# env = video_recorder.VideoRecorder(env, \"./video\")\n",
    "# observation = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf56a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_episode(RL_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821cf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    #your agent goes here\n",
    "    action = random_agent(observation)\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    if done: \n",
    "      break;    \n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extra Food for Thought\n",
    "What if we didn't have a reinforcement learning agent? Can we still solve this problem using normal methods?\n",
    "\n",
    "We can instead use a very simple function to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
