{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dbc69ca5",
      "metadata": {
        "id": "dbc69ca5"
      },
      "source": [
        "# SC3000/CZ3005 Assignment 1 - Balancing a Pole on a Cart"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e0569f",
      "metadata": {
        "id": "09e0569f"
      },
      "source": [
        "Submission Deadline: 11:59 PM, 5 April 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c18b0783",
      "metadata": {
        "id": "c18b0783"
      },
      "source": [
        "## Group Members and Contributions:\n",
        "\n",
        "1. Bryan Toh Wee Sheng\n",
        "2. Lim Shaojun\n",
        "3. Keith Lim En Kai (U2220506C)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36572ef7",
      "metadata": {
        "id": "36572ef7"
      },
      "source": [
        "# 1. Problem description: (Taken from Assignment Document)\n",
        "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on thecart and the goal is to balance the pole by applying forces in the left and right direction on the cart. In this project, you will need to develop a Reinforcement Learning (RL) agent. The trained agent makes the decision to push the cart to the left or right based on the cart position, velocity, and the pole angle, angular velocity. (4 parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "432e883c",
      "metadata": {
        "id": "432e883c"
      },
      "source": [
        "![Alternative Text](https://www.gymlibrary.dev/_images/cart_pole.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd86c998",
      "metadata": {
        "id": "fd86c998"
      },
      "source": [
        "## 1.1 Problem Instance\n",
        "You are given an instance of the cart pole environment implemented by the gym library. As with any good solution to a problem, we start with\n",
        "\n",
        "### Action Space:\n",
        "The action is an nd array with shape (1, which can take values {0,1} indicating actions pushing the cart to the left or right respectively. Note that the velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it.\n",
        "\n",
        "### Observation Space:\n",
        "The observation is an nd array with shape (4,) with the values corresponding to the following positions and velocities:\n",
        "\n",
        "| Num | Observation | Min | Max |\n",
        "|---|---|---|---|\n",
        "| 1 | Cart Position | -4.8 | 4.8 |\n",
        "| 2 | Cart Velocity | -inf | inf |\n",
        "| 3 | Pole Angle | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
        "| 4 | Pole Angular Velocity | -inf | inf |\n",
        "\n",
        "### Reward:\n",
        "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted.\n",
        "\n",
        "### Starting State:\n",
        "All observations are assigned a uniformly random value in **(-0.05, 0.05)**\n",
        "\n",
        "### Episode End:\n",
        "The episode ends if any one of the following occurs:\n",
        "1. Termination: Pole Angle is greater than +/- 12 Degrees == 0.209 rad\n",
        "2. Termination: Cart Position is greater than +/- 2.4 (center of the cart reaches the edge of the display)\n",
        "3. Truncation: Episode length is greater than 500."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "163981e4",
      "metadata": {
        "id": "163981e4"
      },
      "source": [
        "# 2. Requirements and Guidelines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Required Libraries and Dependencies:"
      ],
      "metadata": {
        "id": "QYBT-pM2RYVu"
      },
      "id": "QYBT-pM2RYVu"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3\n",
        "!pip install shimmy\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import RecordVideo\n",
        "gymlogger.set_level(40) #error only\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "import torch\n",
        "import os # for creating directories\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "def show_video():\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                     loop controls style=\"height: 400px;\">\n",
        "                     <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                     </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4AEl_koRVT4",
        "outputId": "92924320-0617-4225-8ed7-8098a70073e3"
      },
      "id": "f4AEl_koRVT4",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3) (12.4.99)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: shimmy in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (1.25.2)\n",
            "Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (0.29.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Cartpole Environment: (Mini Tutorial taken from example code)"
      ],
      "metadata": {
        "id": "0ZW-vLQLrfz4"
      },
      "id": "0ZW-vLQLrfz4"
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Taking a look at the action space\n",
        "print(env.action_space)\n",
        "\n",
        "# Taking a look at the observation space\n",
        "print(env.observation_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbgcQ2LNrwUJ",
        "outputId": "c50e9ab5-49cf-4edc-99f8-2ebd002af9c7"
      },
      "id": "bbgcQ2LNrwUJ",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(2)\n",
            "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the action_space, Discrete(2) means that there are 2 valid discrete actions: 0 and 1. Where 0 represents left while 1 represents right.\n",
        "\n",
        "Based on the observation_space, the first two arrays define the min and max values of the 4 observed values, corresponding to cart position, velocity and pole angle, angular velocity."
      ],
      "metadata": {
        "id": "rpgBwtRIsQwl"
      },
      "id": "rpgBwtRIsQwl"
    },
    {
      "cell_type": "code",
      "source": [
        "# We call each round of the pole-balancing game an \"episode\". At the start of each episode, make sure the environment is reset, which chooses a random initial state,\n",
        "# e.g., pole slightly tilted to the right. This initialization can be achieved by the code below, which returns the observation of the initial state.\n",
        "observation = env.reset()\n",
        "\n",
        "# Taking a look at the initial observations:\n",
        "print(\"Initial Observations: \", observation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zQtM1NMsPsY",
        "outputId": "fea095e3-5c84-46d3-8f35-f6dec4f3866a"
      },
      "id": "2zQtM1NMsPsY",
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Observations:  [ 0.00424547  0.02089479 -0.04617178  0.03780938]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call each round of the pole-balancing game an \"episode\". At the start of each episode, make sure the environment is reset, which chooses a random initial state, e.g., pole slightly tilted to the right. This initialization can be achieved by the code below, which returns the observation of the initial state."
      ],
      "metadata": {
        "id": "NZAUTrl4t5FV"
      },
      "id": "NZAUTrl4t5FV"
    },
    {
      "cell_type": "code",
      "source": [
        "observation, reward, done, info = env.step(0)[:4]\n",
        "print(\"New observations after choosing action 0:\", observation)\n",
        "print(\"Reward for this step:\", reward)\n",
        "print(\"Is this round done?\", done)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZDk4vEIt99E",
        "outputId": "27f0ee87-3107-44f9-e638-29f7f54c4ca9"
      },
      "id": "PZDk4vEIt99E",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New observations after choosing action 0: [ 0.00466336 -0.17353569 -0.0454156   0.31557462]\n",
            "Reward for this step: 1.0\n",
            "Is this round done? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Game Run using Naive Strategy:\n",
        "Now we can play a full round of the game using a naive strategy (always choosing action 0), and show the cumulative reward in the round. Note that reward returned by env.step(*) corresponds to the reward for current step. So we have to accumulate the reward for each step. Clearly, the naive strategy performs poorly by surviving only a dozen of steps."
      ],
      "metadata": {
        "id": "iymmZ00_uGf-"
      },
      "id": "iymmZ00_uGf-"
    },
    {
      "cell_type": "code",
      "source": [
        "observation = env.reset()\n",
        "cumulative_reward = 0\n",
        "done = False\n",
        "while not done:\n",
        "    observation, reward, done, info = env.step(0)[:4]\n",
        "    cumulative_reward += reward\n",
        "print(\"Cumulative reward for this round:\", cumulative_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DkU8EryuNH7",
        "outputId": "c9959ec4-791e-422e-8f1a-8fac320f708e"
      },
      "id": "9DkU8EryuNH7",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative reward for this round: 9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Tasks and Marking Criteria\n",
        "Now that we have taken a look at how the cartpole game works, we can begin to start using Reinforcement Learning Agents to tackle it.\n",
        "\n",
        "### Task 1: Development of an Reinforcement Learning (RL) Agent. (30 Marks)\n",
        "Demonstrate the correctness of the implementation by sampling a random state from the cart pole environment, inputting to the agent, and outputting a chosen action. Print the values of the state and chosen action in the Jupyter Notebook.\n",
        "\n",
        "To tackle this problem, we have decided to go with the PPO approach and built a RL Agent based on it."
      ],
      "metadata": {
        "id": "hX4j4MG8RTF0"
      },
      "id": "hX4j4MG8RTF0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PPO\n"
      ],
      "metadata": {
        "id": "6w2QelDVxQ4Z"
      },
      "id": "6w2QelDVxQ4Z"
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'CartPole-v1'\n",
        "env = gym.make(env_name)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# model = PPO('MlpPolicy', env, verbose=1, device=\"cuda\") # run this if you have an Nvidia GPU installed\n",
        "model = PPO('MlpPolicy', env, verbose=1, device=\"auto\")   # otherwise run this instead"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mScheO5c0T-4",
        "outputId": "1e33fa1c-ecf9-4dc9-fee8-c79b4bcb041f"
      },
      "id": "mScheO5c0T-4",
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# total_timesteps is the number of env.steps(action) being run during training\n",
        "model.learn(total_timesteps=10000, progress_bar=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Al0XbLBWRGHW",
        "outputId": "3847deb9-b23b-43c6-cae5-f44fb1dffa82"
      },
      "id": "Al0XbLBWRGHW",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-73ce1e8b07df>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# total_timesteps is the number of env.steps(action) being run during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 315\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         total_timesteps, callback = self._setup_learn(\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m_setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_episode_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;31m# Retrieve unnormalized observation for saving into the buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mmaybe_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"options\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmaybe_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Seeds and options are only used once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation = env.reset()\n",
        "action, _ = model.predict(observation)\n",
        "print(\"Observation space is: \", observation)\n",
        "print(\"Action taken is: \", action)"
      ],
      "metadata": {
        "id": "MHsvqv4Gv1_5"
      },
      "id": "MHsvqv4Gv1_5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "45fe5c8b",
      "metadata": {
        "id": "45fe5c8b"
      },
      "source": [
        "### Task 2: Demonstrate the effectiveness of the RL Agent (40 Marks)\n",
        "Run for 100 episodes (reset the enviroment at the beginning of each episode) and plot the cumulative reward against all episodes in the Jupyter Notebook. Print the average reward over the 100 episodes. The average reward should be larger than **195**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum_episode_scores = []\n",
        "\n",
        "for episode in range(1, 100):    ## total 100 episodes\n",
        "    score = 0                   ## reward init\n",
        "    obs = env.reset()         ## observations\n",
        "    done = False                ## episode completes will make done True\n",
        "    state = 0\n",
        "\n",
        "    while True:\n",
        "        action = model.predict(obs)[0]\n",
        "        n_state, reward, done, info = env.step(action)      ## apply action\n",
        "        if (abs(n_state[0][0]) > 2.4 or abs(n_state[0][2]) > 0.209):\n",
        "            break\n",
        "        if state == 500:\n",
        "            break\n",
        "        obs = n_state\n",
        "        score += reward\n",
        "        state += 1\n",
        "\n",
        "    print('Episode:', episode, ';   Score:', score)\n",
        "    sum_episode_scores.append(score)\n",
        "\n",
        "\n",
        "print(\"Average score is \", sum(sum_episode_scores) / len(sum_episode_scores))\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Y0fJDxhB0nwG"
      },
      "id": "Y0fJDxhB0nwG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(sum_episode_scores)\n",
        "plt.title(\"Cumulative reward for each episode\")\n",
        "plt.ylabel(\"Cumulative reward\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MEBmcdRLRqq-"
      },
      "id": "MEBmcdRLRqq-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f50c27b1",
      "metadata": {
        "id": "f50c27b1"
      },
      "source": [
        "### Task 3: Render one episode played by the developed RL agent on the Jupyter Notebook (10 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n",
        "observation = env.reset()\n",
        "total = 0\n",
        "state = 0\n",
        "while True:\n",
        "    env.render()\n",
        "    action = model.predict(observation)[0]\n",
        "    n_state, reward, done, info = env.step( int(action))      ## apply action\n",
        "    if (abs(n_state[0]) > 2.4 or abs(n_state[2]) > 0.209):\n",
        "        break\n",
        "    if state == 500:\n",
        "        break\n",
        "    observation = n_state\n",
        "    total += reward\n",
        "    state += 1\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "id": "OtQOaMO_RrAx"
      },
      "id": "OtQOaMO_RrAx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Episode reward is \", total)"
      ],
      "metadata": {
        "id": "18jNZPBB6a6N"
      },
      "id": "18jNZPBB6a6N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5bbc1ef5",
      "metadata": {
        "id": "5bbc1ef5"
      },
      "source": [
        "### Task 4: Format the Jupyter Notebook by including step-by-step instructions and explanations, such that the notebook is easy to follow and run (20 Marks)\n",
        "Include text explanation to demonstrate the originality of your implementation and your understanding of the code. For example, for each task, explain your approach and analyze the output; if you improve an exisiting approach, explain your improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86076dad",
      "metadata": {
        "id": "86076dad"
      },
      "source": [
        "## 2.2 Output Format\n",
        "All codes are to be included in a single Jupyter notebook written in Python (i.e., .ipynb file).\n",
        "1.\tInclude all codes for Task 1-4. Note that the submission is invalid if it only contains the outputs and plots without codes to obtain it.\n",
        "2.\tRun the notebook before submission to save the output in the notebook, i.e., by opening the ipynb file (without running it), one can see the outputs and plots for Task 1-3\n",
        "3.\tMake sure the Jupyter notebook is runnable, i.e., by running each code block sequentially from top to bottom, one can get the results for Task 1-3. The TAs may run your notebook.\n",
        "4.\tUnless you are experienced with Jupyter, it is recommended to modify from the provided Jupyter notebook sample, rather than creating a new one.\n",
        "5.\tIf the developed RL agent is a trainable neural network, submit a .zip file by zipping the trained model parameters (e.g., .pth for PyTorch) and the ipynb file. In this case, your notebook must include the training code and model loading code.\n",
        "6.\tContribution: Please clearly state the contribution of each team member in the beginning of the Jupyter notebook if you have more than one member in your team."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}